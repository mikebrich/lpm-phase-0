{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad2edb4-b1a3-48ea-9103-0ac17fff34a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PAIRING TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170bf1b-f157-4530-9785-8266c6ac6366",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61a7c2-19ac-4036-8974-e271cfb8cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b7c0d4-9f4b-48dc-b832-7e7f0928d3fb",
   "metadata": {},
   "source": [
    "#### Read In ADP Data For Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274bd51-d1e5-43a2-a6b6-a81f153153ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "raw_adp_df = pd.read_excel('inputs/NEW_PunchSourceEmails2023.xls') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928f4cf-1464-44c3-b2b9-df41165fa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "raw_adp_df = raw_adp_df[['Last Name', 'First Name', 'Personal Contact: Personal Email', \n",
    "                         'Time In', 'Pay Code [Timecard]']] #keep these cols\n",
    "raw_adp_df = raw_adp_df.rename(columns={'Personal Contact: Personal Email': 'Email'}) #rename col\n",
    "##raw_adp_df = raw_adp_df[raw_adp_df['Pay Code [Timecard]'] != \"REGSAL\"] #drop managers from the pay data\n",
    "raw_adp_df.loc[:, 'Date'] = raw_adp_df['Time In'].dt.date #make it solely a date col\n",
    "raw_adp_df.drop(columns=['Time In', 'Pay Code [Timecard]'], inplace=True) #then drop orig time in col\n",
    "\n",
    "raw_adp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cadb4fd-1b1f-4160-a42f-97c8e590c03e",
   "metadata": {},
   "source": [
    "#### Read In Scheduler Shift Data For Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f15ad-9e64-4e8c-b732-c23456ffccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "raw_scheduler_df = pd.read_excel('inputs/NEW_SchedulerShifts2023.xlsx') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541be03-2166-4857-b8ee-a67816a32093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "raw_scheduler_df = raw_scheduler_df.drop(columns=[\"epoch_start_time\", \"cover_time\"]) #drop these cols\n",
    "\n",
    "#can drop any rows that have cover_type full and role = 2 \n",
    "# b/c cover_type full either means request in future or was not taken\n",
    "# 2 = manager so not relevant \n",
    "##raw_scheduler_df = raw_scheduler_df[~((raw_scheduler_df['role'] == 2.0) | \n",
    "##                                      (raw_scheduler_df['cover_type'] == 'full'))]\n",
    "\n",
    "raw_scheduler_df['start_time_est'] = raw_scheduler_df['start_time_est'].dt.date #convert to just date \n",
    "raw_scheduler_df['cover_time_est'] = raw_scheduler_df['cover_time_est'].dt.date #convert to just date \n",
    "\n",
    "raw_scheduler_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72848067-19ee-4aee-b3a7-4d6adf075bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the coverers to a new DF\n",
    "cover_sch_df = raw_scheduler_df[raw_scheduler_df['cover_type'].isin(['before', 'after'])][['coverer_name', \n",
    "                                                                                           'coverer_email', \n",
    "                                                                                           'role', \n",
    "                                                                                           'cover_time_est']]\n",
    "cover_sch_df.rename(columns={'coverer_name': 'assignee_name', \n",
    "                             'coverer_email': 'assignee_email', \n",
    "                             'cover_time_est': 'start_time_est'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360cf1a-9a1c-4f52-84a9-fc5df4776f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the original DF with the coverer DF to make one final shift DF\n",
    "all_sch_df = pd.concat([raw_scheduler_df, cover_sch_df], ignore_index=True)\n",
    "all_sch_df.drop(columns=['role', 'coverer_name', 'cover_type', 'cover_time_est', 'coverer_email'], inplace=True) #drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8c925f-fc85-4487-978c-ce49b7f02e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split name into first and last\n",
    "split_names = all_sch_df['assignee_name'].str.split()\n",
    "all_sch_df['First Name'] = split_names.str[0]\n",
    "all_sch_df['Last Name'] = split_names.str[1:].str.join(' ')\n",
    "\n",
    "all_sch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfa4c4a-0e26-4852-93dc-72848161ef7f",
   "metadata": {},
   "source": [
    "#### Define Timeframe Set to All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cd0d5-7ac3-43fd-bd08-c0e16f67e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2023-04-01').date()\n",
    "end_date = pd.Timestamp('2023-08-21').date()\n",
    "\n",
    "sch_timeframe_df = all_sch_df[(all_sch_df['start_time_est'] >= start_date) & \n",
    "                              (all_sch_df['start_time_est'] <= end_date)]\n",
    "\n",
    "adp_timeframe_df = raw_adp_df[(raw_adp_df['Date'] >= start_date) & \n",
    "                              (raw_adp_df['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d32f85-e5bb-4fea-8764-4faf8fca3c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_timeframe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c93c61-2ddd-4dc7-a166-5c6fdf9a4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_timeframe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e316f76-81c5-435d-84c0-030f9dfa7cfc",
   "metadata": {},
   "source": [
    "#### Grouping - Count Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fd7c3-c0d8-4982-8483-6417fe8ab209",
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_grouped_df = adp_timeframe_df.groupby(['Email', 'First Name', 'Last Name']).size().reset_index(name='ADP_Count')\n",
    "adp_grouped_df = adp_grouped_df.sort_values(by='Last Name') #sort by last name\n",
    "adp_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c6913-0d8c-4af6-bf34-21a84f3d015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_grouped_df = sch_timeframe_df.groupby(['assignee_email', 'First Name', 'Last Name']).size().reset_index(name='Sch_Count') #find count per person\n",
    "sch_grouped_df = sch_grouped_df.sort_values(by='Last Name') #sort by last name\n",
    "sch_grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c14d6-6cae-4f66-b06e-01b23d52b4b3",
   "metadata": {},
   "source": [
    "#### Create Pairing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ef720-116c-4a75-8b1b-afe73d0614bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    return name.strip().lower()\n",
    "\n",
    "def normalize_email(email):\n",
    "    return email.strip().lower()\n",
    "\n",
    "#Function to match ADP to Scheduler using a variety of techniques\n",
    "def create_merged_df8(adp_grouped_df, sch_grouped_df):\n",
    "    merged_records = []\n",
    "    unmatched_sch_records = []\n",
    "\n",
    "    for sch_index, sch_row in sch_grouped_df.iterrows():\n",
    "        normalized_assignee_email = normalize_email(sch_row['assignee_email'])\n",
    "\n",
    "        matching_email_rows = adp_grouped_df[adp_grouped_df['Email'].apply(normalize_email) == normalized_assignee_email]\n",
    "        \n",
    "        if not matching_email_rows.empty:\n",
    "            for _, matching_row in matching_email_rows.iterrows():\n",
    "                merged_records.append([\n",
    "                    matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                    sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'email'\n",
    "                ])\n",
    "        else:\n",
    "            # Check for exact first name and last name match\n",
    "            exact_name_match_rows = adp_grouped_df[\n",
    "                (adp_grouped_df['First Name'].apply(normalize_name) == normalize_name(sch_row['First Name'])) &\n",
    "                (adp_grouped_df['Last Name'].apply(normalize_name) == normalize_name(sch_row['Last Name']))\n",
    "            ]\n",
    "            \n",
    "            if not exact_name_match_rows.empty:\n",
    "                for _, matching_row in exact_name_match_rows.iterrows():\n",
    "                    merged_records.append([\n",
    "                        matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                        sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'name_exact'\n",
    "                    ])\n",
    "            else:\n",
    "                # Check for first initial and last name match or first name and last initial match\n",
    "                if sch_row['Last Name']:  # Check if 'Last Name' is not empty\n",
    "                    initial_match_rows = adp_grouped_df[\n",
    "                        ((adp_grouped_df['First Name'].str[0] == sch_row['First Name'][0]) &\n",
    "                         (adp_grouped_df['Last Name'] == sch_row['Last Name'])) |\n",
    "                        ((adp_grouped_df['First Name'] == sch_row['First Name']) &\n",
    "                         (adp_grouped_df['Last Name'].str[0] == sch_row['Last Name'][0]))\n",
    "                    ]\n",
    "                \n",
    "                    if not initial_match_rows.empty:\n",
    "                        for _, matching_row in initial_match_rows.iterrows():\n",
    "                            merged_records.append([\n",
    "                                matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                                sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'name_portion'\n",
    "                            ])\n",
    "                    else:\n",
    "                        unmatched_sch_records.append(sch_row)\n",
    "                else:\n",
    "                    unmatched_sch_records.append(sch_row)  # Handle empty 'Last Name'\n",
    "\n",
    "    # Create a DataFrame from the collected records\n",
    "    merged_df = pd.DataFrame(merged_records, columns=[\n",
    "        'Email_ADP', 'First_Name_ADP', 'Last_Name_ADP', 'ADP_Count',\n",
    "        'Email_Sch', 'First_Name_Sch', 'Last_Name_Sch', 'Sch_Count', 'Matched?'\n",
    "    ])\n",
    "    \n",
    "    # Create a DataFrame for unmatched rows from sch_grouped_df\n",
    "    unmatched_sch_df = pd.DataFrame(unmatched_sch_records, columns=sch_grouped_df.columns)\n",
    "    \n",
    "    # Append unmatched rows to the merged_df with 'none' in the 'Matched?' column\n",
    "    unmatched_rows = pd.DataFrame({\n",
    "        'Email_ADP': None,\n",
    "        'First_Name_ADP': None,\n",
    "        'Last_Name_ADP': None,\n",
    "        'ADP_Count': None,\n",
    "        'Email_Sch': unmatched_sch_df['assignee_email'],\n",
    "        'First_Name_Sch': unmatched_sch_df['First Name'],\n",
    "        'Last_Name_Sch': unmatched_sch_df['Last Name'],\n",
    "        'Sch_Count': unmatched_sch_df['Sch_Count'],\n",
    "        'Matched?': 'none'\n",
    "    })\n",
    "    merged_df = pd.concat([merged_df, unmatched_rows], ignore_index=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1832110-e5db-4c95-a97d-97aa8fbfd713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pairing table\n",
    "pairing_table = create_merged_df8(adp_grouped_df, sch_grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c5e7ea-61e4-414e-a95f-7fad6f4d002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Manual changes\n",
    "email_to_change = \"dalessionicholas6@gmail.com\"\n",
    "sch_email_to_match = \"dalessnp@dukes.jmu.edu\"\n",
    "\n",
    "pairing_table.loc[pairing_table['Email_Sch'] == sch_email_to_match, 'Email_ADP'] = email_to_change\n",
    "pairing_table.loc[pairing_table['Email_Sch'] == sch_email_to_match, 'Matched?'] = 'manual'\n",
    "\n",
    "pairing_table = pairing_table[[\"Email_ADP\", \"Email_Sch\", \"Matched?\"]] #keep relevant cols\n",
    "pairing_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be54f29-6930-462a-8e4d-7af526361be1",
   "metadata": {},
   "source": [
    "## ADOPTION CALC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501ff4c-5722-41dc-bc9d-dbf72c2e110b",
   "metadata": {},
   "source": [
    "#### Read in ADP Data For Adoption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd677a4-342a-4a50-8f16-2e8a4d60058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "adp_adoption = pd.read_excel('inputs/NEW_PunchSourceEmails2023.xls') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626735ab-a14c-4100-ab36-a7d990a13728",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "adp_adoption = adp_adoption[['Last Name', 'First Name', 'Personal Contact: Personal Email', \n",
    "                         'Time In', 'Pay Code [Timecard]']] #keep these cols\n",
    "adp_adoption = adp_adoption.rename(columns={'Personal Contact: Personal Email': 'Email'}) #rename col\n",
    "adp_adoption = adp_adoption[adp_adoption['Pay Code [Timecard]'] != \"REGSAL\"] #drop managers from the pay data\n",
    "adp_adoption.loc[:, 'Date'] = adp_adoption['Time In'].dt.date #make it solely a date col\n",
    "adp_adoption.drop(columns=['Time In', 'Pay Code [Timecard]'], inplace=True) #then drop orig time in col\n",
    "\n",
    "adp_adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b84cb2-dcbe-41b3-9664-3fc089987960",
   "metadata": {},
   "source": [
    "#### Read in Scheduler Data For Adoption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec2f10-e183-4364-abf8-e67a1b87eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "scheduler_adoption = pd.read_excel('inputs/NEW_SchedulerShifts2023.xlsx') #read in the excel, put into DF\n",
    "scheduler_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4ebd1-64d5-4feb-97da-fc46dcc3aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "scheduler_adoption = scheduler_adoption.drop(columns=[\"epoch_start_time\", \"cover_time\"]) #drop these cols\n",
    "\n",
    "#can drop any rows that have cover_type full and role = 2 \n",
    "# b/c cover_type full either means request in future or was not taken\n",
    "# 2 = manager so not relevant \n",
    "scheduler_adoption = scheduler_adoption[~((scheduler_adoption['role'] == 2.0) | \n",
    "                                          (scheduler_adoption['cover_type'] == 'full'))]\n",
    "\n",
    "scheduler_adoption['start_time_est'] = scheduler_adoption['start_time_est'].dt.date #convert to just date \n",
    "scheduler_adoption['cover_time_est'] = scheduler_adoption['cover_time_est'].dt.date #convert to just date \n",
    "\n",
    "scheduler_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6874f4e2-715b-4610-b30a-ff2b7a07147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the coverers to a new DF\n",
    "cover_scheduler_adoption = scheduler_adoption[scheduler_adoption['cover_type'].isin(['before', 'after'])][['coverer_name', \n",
    "                                                                                           'coverer_email', \n",
    "                                                                                           'role', \n",
    "                                                                                           'cover_time_est']]\n",
    "cover_scheduler_adoption.rename(columns={'coverer_name': 'assignee_name', \n",
    "                             'coverer_email': 'assignee_email', \n",
    "                             'cover_time_est': 'start_time_est'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9bd17-18c2-4fb4-97ae-97107384962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the original DF with the coverer DF to make one final shift DF\n",
    "final_scheduler_adoption = pd.concat([scheduler_adoption, cover_scheduler_adoption], ignore_index=True)\n",
    "final_scheduler_adoption.drop(columns=['role', 'coverer_name', 'cover_type', 'cover_time_est', 'coverer_email'], inplace=True) #drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ec927-48a0-4256-a9d9-b98a41955240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split name into first and last\n",
    "split_names = final_scheduler_adoption['assignee_name'].str.split()\n",
    "final_scheduler_adoption['First Name'] = split_names.str[0]\n",
    "final_scheduler_adoption['Last Name'] = split_names.str[1:].str.join(' ')\n",
    "\n",
    "final_scheduler_adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7686b2-63fd-41c4-bbbe-78c5db106e65",
   "metadata": {},
   "source": [
    "#### Define Timeframe For Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f164458-ea03-4dcf-a241-23036a863514",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2023-06-01').date()\n",
    "end_date = pd.Timestamp('2023-06-07').date()\n",
    "\n",
    "TF_SCH = final_scheduler_adoption[(final_scheduler_adoption['start_time_est'] >= start_date) & \n",
    "                                  (final_scheduler_adoption['start_time_est'] <= end_date)]\n",
    "\n",
    "TF_ADP = adp_adoption[(adp_adoption['Date'] >= start_date) & \n",
    "                      (adp_adoption['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5924f32-6401-4cb3-b51c-c2622011a724",
   "metadata": {},
   "source": [
    "#### Grouping -- Count Shifts For Adoption Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b2c7c-b6a8-4480-bdff-3f4d6ff730e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_ADP = TF_ADP.groupby(['Email']).size().reset_index(name='ADP_Count')\n",
    "GR_ADP.rename(columns={\"Email\": \"Email_ADP\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb81897-cda8-487d-91d8-352f6e49ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_SCH = TF_SCH.groupby(['assignee_email']).size().reset_index(name='Sch_Count') #find count per person\n",
    "GR_SCH.rename(columns={\"assignee_email\": \"Email_Sch\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f0e8c0-e3a3-402d-b722-7837cd83f7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = GR_SCH.merge(pairing_table, on=\"Email_Sch\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e137581-ae54-4afe-89c1-7deb82b30e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = counting.merge(GR_ADP, on=\"Email_ADP\", how=\"left\")\n",
    "\n",
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f832b-3801-478b-bb0d-ee51e185e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Column to Measure difference in shifts\n",
    "#Sch - ADP;\n",
    "# PLUS means more SCH Shifts (relatively good)\n",
    "# MINUS means more ADP shifts (relatively bad)\n",
    "\n",
    "counting[\"Diff\"] = np.where(\n",
    "    counting[\"Sch_Count\"].notna() & counting[\"ADP_Count\"].notna(),\n",
    "    counting[\"Sch_Count\"] - counting[\"ADP_Count\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fe8d8-ec10-4786-a19e-5ec6d616ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_within_range = counting[\"Diff\"].between(-2, 2).sum()\n",
    "count_within_range "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241e834-104e-40f6-9a78-07a111adae85",
   "metadata": {},
   "source": [
    "#### Calculate Adoption Rates Per Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f032bc-a07b-456d-ad70-26c7c4024bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adoption_rates_per_week(start_date, end_date, ADP_DF, SCH_DF, pairing_table):\n",
    "    \n",
    "    #Filter the main ADP and SCH dataframes to the respective timeframes\n",
    "    ADP_DF = ADP_DF[(ADP_DF['Date'] >= start_date) & \n",
    "                    (ADP_DF['Date'] <= end_date)]\n",
    "    \n",
    "    SCH_DF = SCH_DF[(SCH_DF['start_time_est'] >= start_date) & \n",
    "                    (SCH_DF['start_time_est'] <= end_date)]\n",
    "\n",
    "    #Group the ADP and SCH DFs by email\n",
    "    GR_ADP = ADP_DF.groupby(['Email']).size().reset_index(name='ADP_Count')\n",
    "    GR_ADP.rename(columns={\"Email\": \"Email_ADP\"}, inplace=True)\n",
    "    \n",
    "    GR_SCH = SCH_DF.groupby(['assignee_email']).size().reset_index(name='SCH_Count')\n",
    "    GR_SCH.rename(columns={\"assignee_email\": \"Email_Sch\"}, inplace=True)\n",
    "    \n",
    "    #Match the emails with the pairing table\n",
    "    matched_df = GR_SCH.merge(pairing_table, on=\"Email_Sch\", how=\"left\")\n",
    "    matched_df = matched_df.merge(GR_ADP, on=\"Email_ADP\", how=\"left\")\n",
    "    \n",
    "    #Create Column to Measure difference in shifts (plus means more SCH shiftS)\n",
    "    matched_df[\"Diff\"] = np.where(\n",
    "        matched_df[\"SCH_Count\"].notna() & matched_df[\"ADP_Count\"].notna(),\n",
    "        matched_df[\"SCH_Count\"] - matched_df[\"ADP_Count\"],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    #Calculate the yes/partial/no counts\n",
    "    adoption_yes_count = matched_df[\"Diff\"].between(-2, 2).sum()\n",
    "    adoption_partial_count = matched_df.shape[0] - adoption_yes_count\n",
    "    adoption_no_count = GR_ADP.shape[0] - adoption_yes_count - adoption_partial_count\n",
    "    sum_count = adoption_yes_count + adoption_partial_count + adoption_no_count\n",
    "    \n",
    "    #Calculate the yes/partial/no percents\n",
    "    yes_perc = round(adoption_yes_count / sum_count, 4)\n",
    "    partial_perc = round(adoption_partial_count / sum_count, 4)\n",
    "    no_perc = round(adoption_no_count / sum_count, 4)\n",
    "    \n",
    "    #Format start/end date\n",
    "    formatted_start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    formatted_end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return [formatted_start_date, formatted_end_date, yes_perc, partial_perc, no_perc, sum_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f59fa1-3e53-4202-805f-b1272f279f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "date_ranges = [\n",
    "    (pd.Timestamp('2023-06-01').date(), pd.Timestamp('2023-06-07').date()),\n",
    "    (pd.Timestamp('2023-06-08').date(), pd.Timestamp('2023-06-14').date()),\n",
    "    (pd.Timestamp('2023-06-15').date(), pd.Timestamp('2023-06-21').date()),\n",
    "    (pd.Timestamp('2023-06-22').date(), pd.Timestamp('2023-06-28').date()),\n",
    "    (pd.Timestamp('2023-06-29').date(), pd.Timestamp('2023-07-05').date()),\n",
    "    (pd.Timestamp('2023-07-06').date(), pd.Timestamp('2023-07-12').date()),\n",
    "    (pd.Timestamp('2023-07-13').date(), pd.Timestamp('2023-07-19').date()),\n",
    "    (pd.Timestamp('2023-07-20').date(), pd.Timestamp('2023-07-26').date()),\n",
    "    (pd.Timestamp('2023-07-27').date(), pd.Timestamp('2023-08-02').date()),\n",
    "    (pd.Timestamp('2023-08-03').date(), pd.Timestamp('2023-08-09').date()),\n",
    "    (pd.Timestamp('2023-08-10').date(), pd.Timestamp('2023-08-16').date()),\n",
    "    (pd.Timestamp('2023-08-17').date(), pd.Timestamp('2023-08-23').date()),\n",
    "    (pd.Timestamp('2023-08-24').date(), pd.Timestamp('2023-08-30').date()), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23fa84-512b-4eae-ac3e-b7fb5b8f8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "date_ranges = [\n",
    "    \n",
    "    (pd.Timestamp('2023-06-01').date(), pd.Timestamp('2023-06-01').date()),\n",
    "    (pd.Timestamp('2023-06-02').date(), pd.Timestamp('2023-06-02').date()),\n",
    "    (pd.Timestamp('2023-06-03').date(), pd.Timestamp('2023-06-03').date()),\n",
    "    (pd.Timestamp('2023-06-04').date(), pd.Timestamp('2023-06-04').date()),\n",
    " #   (pd.Timestamp('2023-06-05').date(), pd.Timestamp('2023-06-05').date()),\n",
    "    (pd.Timestamp('2023-06-06').date(), pd.Timestamp('2023-06-06').date()),\n",
    " #   (pd.Timestamp('2023-06-07').date(), pd.Timestamp('2023-06-07').date()),\n",
    "    (pd.Timestamp('2023-06-08').date(), pd.Timestamp('2023-06-08').date()),\n",
    "    (pd.Timestamp('2023-06-09').date(), pd.Timestamp('2023-06-09').date()),\n",
    "    (pd.Timestamp('2023-06-10').date(), pd.Timestamp('2023-06-10').date()),\n",
    "    (pd.Timestamp('2023-06-11').date(), pd.Timestamp('2023-06-11').date()),\n",
    "    (pd.Timestamp('2023-06-12').date(), pd.Timestamp('2023-06-12').date()),\n",
    "    (pd.Timestamp('2023-06-13').date(), pd.Timestamp('2023-06-13').date()),\n",
    "    (pd.Timestamp('2023-06-14').date(), pd.Timestamp('2023-06-14').date()),\n",
    "    (pd.Timestamp('2023-06-15').date(), pd.Timestamp('2023-06-15').date()),\n",
    "    (pd.Timestamp('2023-06-16').date(), pd.Timestamp('2023-06-16').date()),\n",
    "    (pd.Timestamp('2023-06-17').date(), pd.Timestamp('2023-06-17').date()),\n",
    "    (pd.Timestamp('2023-06-18').date(), pd.Timestamp('2023-06-18').date()),\n",
    "    (pd.Timestamp('2023-06-19').date(), pd.Timestamp('2023-06-19').date()),\n",
    "    (pd.Timestamp('2023-06-20').date(), pd.Timestamp('2023-06-20').date()),\n",
    "    (pd.Timestamp('2023-06-21').date(), pd.Timestamp('2023-06-21').date()),\n",
    "    (pd.Timestamp('2023-06-22').date(), pd.Timestamp('2023-06-22').date()),\n",
    "  #  (pd.Timestamp('2023-06-23').date(), pd.Timestamp('2023-06-23').date()),\n",
    "    (pd.Timestamp('2023-06-24').date(), pd.Timestamp('2023-06-24').date()),\n",
    "    (pd.Timestamp('2023-06-25').date(), pd.Timestamp('2023-06-25').date()),\n",
    "    (pd.Timestamp('2023-06-26').date(), pd.Timestamp('2023-06-26').date()),\n",
    "    (pd.Timestamp('2023-06-27').date(), pd.Timestamp('2023-06-27').date()),\n",
    "    (pd.Timestamp('2023-06-28').date(), pd.Timestamp('2023-06-28').date()),\n",
    "    (pd.Timestamp('2023-06-29').date(), pd.Timestamp('2023-06-29').date()),\n",
    "    (pd.Timestamp('2023-06-30').date(), pd.Timestamp('2023-06-30').date()),\n",
    "        \n",
    "    (pd.Timestamp('2023-07-01').date(), pd.Timestamp('2023-07-01').date()),\n",
    "    (pd.Timestamp('2023-07-02').date(), pd.Timestamp('2023-07-02').date()),\n",
    "    (pd.Timestamp('2023-07-03').date(), pd.Timestamp('2023-07-03').date()),\n",
    "    (pd.Timestamp('2023-07-04').date(), pd.Timestamp('2023-07-04').date()),\n",
    "    (pd.Timestamp('2023-07-05').date(), pd.Timestamp('2023-07-05').date()),\n",
    "    (pd.Timestamp('2023-07-06').date(), pd.Timestamp('2023-07-06').date()),\n",
    "    (pd.Timestamp('2023-07-07').date(), pd.Timestamp('2023-07-07').date()),\n",
    "    (pd.Timestamp('2023-07-08').date(), pd.Timestamp('2023-07-08').date()),\n",
    "    (pd.Timestamp('2023-07-09').date(), pd.Timestamp('2023-07-09').date()),\n",
    "    (pd.Timestamp('2023-07-10').date(), pd.Timestamp('2023-07-10').date()),\n",
    "    (pd.Timestamp('2023-07-11').date(), pd.Timestamp('2023-07-11').date()),\n",
    "    (pd.Timestamp('2023-07-12').date(), pd.Timestamp('2023-07-12').date()),\n",
    "    (pd.Timestamp('2023-07-13').date(), pd.Timestamp('2023-07-13').date()),\n",
    "    (pd.Timestamp('2023-07-14').date(), pd.Timestamp('2023-07-14').date()),\n",
    "    (pd.Timestamp('2023-07-15').date(), pd.Timestamp('2023-07-15').date()),\n",
    " #   (pd.Timestamp('2023-07-16').date(), pd.Timestamp('2023-07-16').date()),\n",
    "    (pd.Timestamp('2023-07-17').date(), pd.Timestamp('2023-07-17').date()),\n",
    "    (pd.Timestamp('2023-07-18').date(), pd.Timestamp('2023-07-18').date()),\n",
    "    (pd.Timestamp('2023-07-19').date(), pd.Timestamp('2023-07-19').date()),\n",
    "    (pd.Timestamp('2023-07-20').date(), pd.Timestamp('2023-07-20').date()),\n",
    "    (pd.Timestamp('2023-07-21').date(), pd.Timestamp('2023-07-21').date()),\n",
    "    (pd.Timestamp('2023-07-22').date(), pd.Timestamp('2023-07-22').date()),\n",
    "    (pd.Timestamp('2023-07-23').date(), pd.Timestamp('2023-07-23').date()),\n",
    "    (pd.Timestamp('2023-07-24').date(), pd.Timestamp('2023-07-24').date()),\n",
    "    (pd.Timestamp('2023-07-25').date(), pd.Timestamp('2023-07-25').date()),\n",
    "    (pd.Timestamp('2023-07-26').date(), pd.Timestamp('2023-07-26').date()),\n",
    "    (pd.Timestamp('2023-07-27').date(), pd.Timestamp('2023-07-27').date()),\n",
    "    (pd.Timestamp('2023-07-28').date(), pd.Timestamp('2023-07-28').date()),\n",
    "    (pd.Timestamp('2023-07-29').date(), pd.Timestamp('2023-07-29').date()),\n",
    "    (pd.Timestamp('2023-07-30').date(), pd.Timestamp('2023-07-30').date()),\n",
    "    (pd.Timestamp('2023-07-31').date(), pd.Timestamp('2023-07-31').date()),\n",
    "    \n",
    "    (pd.Timestamp('2023-08-01').date(), pd.Timestamp('2023-08-01').date()),\n",
    "    (pd.Timestamp('2023-08-02').date(), pd.Timestamp('2023-08-02').date()),\n",
    "    (pd.Timestamp('2023-08-03').date(), pd.Timestamp('2023-08-03').date()),\n",
    "    (pd.Timestamp('2023-08-04').date(), pd.Timestamp('2023-08-04').date()),\n",
    "    (pd.Timestamp('2023-08-05').date(), pd.Timestamp('2023-08-05').date()),\n",
    "    (pd.Timestamp('2023-08-06').date(), pd.Timestamp('2023-08-06').date()),\n",
    "    (pd.Timestamp('2023-08-07').date(), pd.Timestamp('2023-08-07').date()),\n",
    "    (pd.Timestamp('2023-08-08').date(), pd.Timestamp('2023-08-08').date()),\n",
    "    (pd.Timestamp('2023-08-09').date(), pd.Timestamp('2023-08-09').date()),\n",
    "    (pd.Timestamp('2023-08-10').date(), pd.Timestamp('2023-08-10').date()),\n",
    "    (pd.Timestamp('2023-08-11').date(), pd.Timestamp('2023-08-11').date()),\n",
    "    (pd.Timestamp('2023-08-12').date(), pd.Timestamp('2023-08-12').date()),\n",
    "    (pd.Timestamp('2023-08-13').date(), pd.Timestamp('2023-08-13').date()),\n",
    "    (pd.Timestamp('2023-08-14').date(), pd.Timestamp('2023-08-14').date()),\n",
    "    (pd.Timestamp('2023-08-15').date(), pd.Timestamp('2023-08-15').date()),\n",
    "    (pd.Timestamp('2023-08-16').date(), pd.Timestamp('2023-08-16').date()),\n",
    "    (pd.Timestamp('2023-08-17').date(), pd.Timestamp('2023-08-17').date()),\n",
    "    (pd.Timestamp('2023-08-18').date(), pd.Timestamp('2023-08-18').date()),\n",
    "    (pd.Timestamp('2023-08-19').date(), pd.Timestamp('2023-08-19').date()),\n",
    "    (pd.Timestamp('2023-08-20').date(), pd.Timestamp('2023-08-20').date()),\n",
    "    (pd.Timestamp('2023-08-21').date(), pd.Timestamp('2023-08-21').date()),\n",
    "    (pd.Timestamp('2023-08-22').date(), pd.Timestamp('2023-08-22').date()),\n",
    "    (pd.Timestamp('2023-08-23').date(), pd.Timestamp('2023-08-23').date()),\n",
    "    (pd.Timestamp('2023-08-24').date(), pd.Timestamp('2023-08-24').date()),\n",
    "    (pd.Timestamp('2023-08-25').date(), pd.Timestamp('2023-08-25').date()),\n",
    "    (pd.Timestamp('2023-08-26').date(), pd.Timestamp('2023-08-26').date()),\n",
    "    (pd.Timestamp('2023-08-27').date(), pd.Timestamp('2023-08-27').date()),\n",
    "    (pd.Timestamp('2023-08-28').date(), pd.Timestamp('2023-08-28').date()),\n",
    "    (pd.Timestamp('2023-08-29').date(), pd.Timestamp('2023-08-29').date()),\n",
    "    (pd.Timestamp('2023-08-30').date(), pd.Timestamp('2023-08-30').date()),\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd05364-1739-4db2-90ef-c458785a500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for start_date, end_date in date_ranges:\n",
    "    week_result = adoption_rates_per_week(start_date, \n",
    "                                          end_date, \n",
    "                                          adp_adoption, \n",
    "                                          final_scheduler_adoption, \n",
    "                                          pairing_table)\n",
    "    results.append(week_result)\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\"Start Date\", \"End Date\", \"Yes %\", \"Partial %\", \"No %\", \"Sum Users\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b7776b-3f81-4ca1-8293-dbd1eedabdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5765c9-f121-496b-86fa-41f924a9e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Melt the DataFrame to make it suitable for plotting\n",
    "melted_df = result_df.melt(id_vars=['Start Date', 'End Date'], value_vars=['Yes %', 'Partial %', 'No %'],\n",
    "                    var_name='Percentage', value_name='Value')\n",
    "\n",
    "# Define custom hex colors for each percentage category\n",
    "color_map = {\n",
    "    'Yes %': '#44B699',\n",
    "    'Partial %': '#90EE90',\n",
    "    'No %': '#EDEDED',\n",
    "}\n",
    "\n",
    "# Create the stacked bar chart using Plotly Express with custom colors\n",
    "fig = px.bar(melted_df, x='Start Date', y='Value', color='Percentage',\n",
    "             title=\"Stacked Bar Chart of Yes%, Partial%, and No%\",\n",
    "             labels={'Value': 'Percentage'},\n",
    "             color_discrete_map=color_map,  # Use custom color map\n",
    "             height=500)\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis_tickformat='.0%',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "# Customize hover behavior\n",
    "hover_template = '<b>%{x}</b><br>%{y:.0%}<extra></extra>'\n",
    "fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "\n",
    "# Lighten the colors of other bars when hovering over yellow section\n",
    "for color_key in color_map:\n",
    "    color_value = color_map[color_key]\n",
    "    fig.update_traces(selector=dict(marker_color=[color_value]),\n",
    "                      unselected=dict(marker_opacity=0.3))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57873147-ab51-49fe-8baf-d7b9be42ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "date_ranges = [\n",
    "    \n",
    "    (pd.Timestamp('2023-06-01').date(), pd.Timestamp('2023-06-01').date()),\n",
    "    (pd.Timestamp('2023-06-02').date(), pd.Timestamp('2023-06-02').date()),\n",
    "    (pd.Timestamp('2023-06-03').date(), pd.Timestamp('2023-06-03').date()),\n",
    "    (pd.Timestamp('2023-06-04').date(), pd.Timestamp('2023-06-04').date()),\n",
    " #   (pd.Timestamp('2023-06-05').date(), pd.Timestamp('2023-06-05').date()),\n",
    "    (pd.Timestamp('2023-06-06').date(), pd.Timestamp('2023-06-06').date()),\n",
    " #   (pd.Timestamp('2023-06-07').date(), pd.Timestamp('2023-06-07').date()),\n",
    "    (pd.Timestamp('2023-06-08').date(), pd.Timestamp('2023-06-08').date()),\n",
    "    (pd.Timestamp('2023-06-09').date(), pd.Timestamp('2023-06-09').date()),\n",
    "    (pd.Timestamp('2023-06-10').date(), pd.Timestamp('2023-06-10').date()),\n",
    "    (pd.Timestamp('2023-06-11').date(), pd.Timestamp('2023-06-11').date()),\n",
    "    (pd.Timestamp('2023-06-12').date(), pd.Timestamp('2023-06-12').date()),\n",
    "    (pd.Timestamp('2023-06-13').date(), pd.Timestamp('2023-06-13').date()),\n",
    "    (pd.Timestamp('2023-06-14').date(), pd.Timestamp('2023-06-14').date()),\n",
    "    (pd.Timestamp('2023-06-15').date(), pd.Timestamp('2023-06-15').date()),\n",
    "    (pd.Timestamp('2023-06-16').date(), pd.Timestamp('2023-06-16').date()),\n",
    "    (pd.Timestamp('2023-06-17').date(), pd.Timestamp('2023-06-17').date()),\n",
    "    (pd.Timestamp('2023-06-18').date(), pd.Timestamp('2023-06-18').date()),\n",
    "    (pd.Timestamp('2023-06-19').date(), pd.Timestamp('2023-06-19').date()),\n",
    "    (pd.Timestamp('2023-06-20').date(), pd.Timestamp('2023-06-20').date()),\n",
    "    (pd.Timestamp('2023-06-21').date(), pd.Timestamp('2023-06-21').date()),\n",
    "    (pd.Timestamp('2023-06-22').date(), pd.Timestamp('2023-06-22').date()),\n",
    "  #  (pd.Timestamp('2023-06-23').date(), pd.Timestamp('2023-06-23').date()),\n",
    "    (pd.Timestamp('2023-06-24').date(), pd.Timestamp('2023-06-24').date()),\n",
    "    (pd.Timestamp('2023-06-25').date(), pd.Timestamp('2023-06-25').date()),\n",
    "    (pd.Timestamp('2023-06-26').date(), pd.Timestamp('2023-06-26').date()),\n",
    "    (pd.Timestamp('2023-06-27').date(), pd.Timestamp('2023-06-27').date()),\n",
    "    (pd.Timestamp('2023-06-28').date(), pd.Timestamp('2023-06-28').date()),\n",
    "    (pd.Timestamp('2023-06-29').date(), pd.Timestamp('2023-06-29').date()),\n",
    "    (pd.Timestamp('2023-06-30').date(), pd.Timestamp('2023-06-30').date()),\n",
    "        \n",
    "    (pd.Timestamp('2023-07-01').date(), pd.Timestamp('2023-07-01').date()),\n",
    "    (pd.Timestamp('2023-07-02').date(), pd.Timestamp('2023-07-02').date()),\n",
    "    (pd.Timestamp('2023-07-03').date(), pd.Timestamp('2023-07-03').date()),\n",
    "    (pd.Timestamp('2023-07-04').date(), pd.Timestamp('2023-07-04').date()),\n",
    "    (pd.Timestamp('2023-07-05').date(), pd.Timestamp('2023-07-05').date()),\n",
    "    (pd.Timestamp('2023-07-06').date(), pd.Timestamp('2023-07-06').date()),\n",
    "    (pd.Timestamp('2023-07-07').date(), pd.Timestamp('2023-07-07').date()),\n",
    "    (pd.Timestamp('2023-07-08').date(), pd.Timestamp('2023-07-08').date()),\n",
    "    (pd.Timestamp('2023-07-09').date(), pd.Timestamp('2023-07-09').date()),\n",
    "    (pd.Timestamp('2023-07-10').date(), pd.Timestamp('2023-07-10').date()),\n",
    "    (pd.Timestamp('2023-07-11').date(), pd.Timestamp('2023-07-11').date()),\n",
    "    (pd.Timestamp('2023-07-12').date(), pd.Timestamp('2023-07-12').date()),\n",
    "    (pd.Timestamp('2023-07-13').date(), pd.Timestamp('2023-07-13').date()),\n",
    "    (pd.Timestamp('2023-07-14').date(), pd.Timestamp('2023-07-14').date()),\n",
    "    (pd.Timestamp('2023-07-15').date(), pd.Timestamp('2023-07-15').date()),\n",
    " #   (pd.Timestamp('2023-07-16').date(), pd.Timestamp('2023-07-16').date()),\n",
    "    (pd.Timestamp('2023-07-17').date(), pd.Timestamp('2023-07-17').date()),\n",
    "    (pd.Timestamp('2023-07-18').date(), pd.Timestamp('2023-07-18').date()),\n",
    "    (pd.Timestamp('2023-07-19').date(), pd.Timestamp('2023-07-19').date()),\n",
    "    (pd.Timestamp('2023-07-20').date(), pd.Timestamp('2023-07-20').date()),\n",
    "    (pd.Timestamp('2023-07-21').date(), pd.Timestamp('2023-07-21').date()),\n",
    "    (pd.Timestamp('2023-07-22').date(), pd.Timestamp('2023-07-22').date()),\n",
    "    (pd.Timestamp('2023-07-23').date(), pd.Timestamp('2023-07-23').date()),\n",
    "    (pd.Timestamp('2023-07-24').date(), pd.Timestamp('2023-07-24').date()),\n",
    "    (pd.Timestamp('2023-07-25').date(), pd.Timestamp('2023-07-25').date()),\n",
    "    (pd.Timestamp('2023-07-26').date(), pd.Timestamp('2023-07-26').date()),\n",
    "    (pd.Timestamp('2023-07-27').date(), pd.Timestamp('2023-07-27').date()),\n",
    "    (pd.Timestamp('2023-07-28').date(), pd.Timestamp('2023-07-28').date()),\n",
    "    (pd.Timestamp('2023-07-29').date(), pd.Timestamp('2023-07-29').date()),\n",
    "    (pd.Timestamp('2023-07-30').date(), pd.Timestamp('2023-07-30').date()),\n",
    "    (pd.Timestamp('2023-07-31').date(), pd.Timestamp('2023-07-31').date()),\n",
    "    \n",
    "    (pd.Timestamp('2023-08-01').date(), pd.Timestamp('2023-08-01').date()),\n",
    "    (pd.Timestamp('2023-08-02').date(), pd.Timestamp('2023-08-02').date()),\n",
    "    (pd.Timestamp('2023-08-03').date(), pd.Timestamp('2023-08-03').date()),\n",
    "    (pd.Timestamp('2023-08-04').date(), pd.Timestamp('2023-08-04').date()),\n",
    "    (pd.Timestamp('2023-08-05').date(), pd.Timestamp('2023-08-05').date()),\n",
    "    (pd.Timestamp('2023-08-06').date(), pd.Timestamp('2023-08-06').date()),\n",
    "    (pd.Timestamp('2023-08-07').date(), pd.Timestamp('2023-08-07').date()),\n",
    "    (pd.Timestamp('2023-08-08').date(), pd.Timestamp('2023-08-08').date()),\n",
    "    (pd.Timestamp('2023-08-09').date(), pd.Timestamp('2023-08-09').date()),\n",
    "    (pd.Timestamp('2023-08-10').date(), pd.Timestamp('2023-08-10').date()),\n",
    "    (pd.Timestamp('2023-08-11').date(), pd.Timestamp('2023-08-11').date()),\n",
    "    (pd.Timestamp('2023-08-12').date(), pd.Timestamp('2023-08-12').date()),\n",
    "    (pd.Timestamp('2023-08-13').date(), pd.Timestamp('2023-08-13').date()),\n",
    "    (pd.Timestamp('2023-08-14').date(), pd.Timestamp('2023-08-14').date()),\n",
    "    (pd.Timestamp('2023-08-15').date(), pd.Timestamp('2023-08-15').date()),\n",
    "    (pd.Timestamp('2023-08-16').date(), pd.Timestamp('2023-08-16').date()),\n",
    "    (pd.Timestamp('2023-08-17').date(), pd.Timestamp('2023-08-17').date()),\n",
    "    (pd.Timestamp('2023-08-18').date(), pd.Timestamp('2023-08-18').date()),\n",
    "    (pd.Timestamp('2023-08-19').date(), pd.Timestamp('2023-08-19').date()),\n",
    "    (pd.Timestamp('2023-08-20').date(), pd.Timestamp('2023-08-20').date()),\n",
    "    (pd.Timestamp('2023-08-21').date(), pd.Timestamp('2023-08-21').date()),\n",
    "    (pd.Timestamp('2023-08-22').date(), pd.Timestamp('2023-08-22').date()),\n",
    "    (pd.Timestamp('2023-08-23').date(), pd.Timestamp('2023-08-23').date()),\n",
    "    (pd.Timestamp('2023-08-24').date(), pd.Timestamp('2023-08-24').date()),\n",
    "    (pd.Timestamp('2023-08-25').date(), pd.Timestamp('2023-08-25').date()),\n",
    "    (pd.Timestamp('2023-08-26').date(), pd.Timestamp('2023-08-26').date()),\n",
    "    (pd.Timestamp('2023-08-27').date(), pd.Timestamp('2023-08-27').date()),\n",
    "    (pd.Timestamp('2023-08-28').date(), pd.Timestamp('2023-08-28').date()),\n",
    "    (pd.Timestamp('2023-08-29').date(), pd.Timestamp('2023-08-29').date()),\n",
    "    (pd.Timestamp('2023-08-30').date(), pd.Timestamp('2023-08-30').date()),\n",
    "] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
