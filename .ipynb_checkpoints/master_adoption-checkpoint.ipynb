{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d431fd-bb96-42ca-ab06-0b8d9332a164",
   "metadata": {},
   "source": [
    "## PAIRING TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bc1ba-8f33-41d6-b805-9a1ba84ad538",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a84316-21ff-4e81-8324-b68ccc74e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc84d42-ef69-4be2-9df7-267b38019e13",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read In ADP Data For Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21407edf-b995-4a3e-a5dd-3ea518b3ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "raw_adp_df = pd.read_excel('inputs/PunchSourceEmails2023.xlsx') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b380c6c-7d0e-4bde-a7ab-0d404335a112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "raw_adp_df = raw_adp_df[['Last Name', 'First Name', 'Personal Contact: Personal Email', \n",
    "                         'Time In', 'Pay Code [Timecard]']] #keep these cols\n",
    "raw_adp_df = raw_adp_df.rename(columns={'Personal Contact: Personal Email': 'Email'}) #rename col\n",
    "##raw_adp_df = raw_adp_df[raw_adp_df['Pay Code [Timecard]'] != \"REGSAL\"] #drop managers from the pay data\n",
    "raw_adp_df.loc[:, 'Date'] = raw_adp_df['Time In'].dt.date #make it solely a date col\n",
    "raw_adp_df.drop(columns=['Time In', 'Pay Code [Timecard]'], inplace=True) #then drop orig time in col\n",
    "\n",
    "raw_adp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20901320-03cf-471d-8660-ec1aa5a66596",
   "metadata": {},
   "source": [
    "#### Read In Scheduler Shift Data For Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e881802b-3bdd-4b74-8277-8f1f2954fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "raw_scheduler_df = pd.read_excel('inputs/SchedulerShifts2023.xlsx') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fbae4-1fc9-427c-823e-dd91f626ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "raw_scheduler_df = raw_scheduler_df.drop(columns=[\"epoch_start_time\", \"cover_time\"]) #drop these cols\n",
    "\n",
    "#can drop any rows that have cover_type full and role = 2 \n",
    "# b/c cover_type full either means request in future or was not taken\n",
    "# 2 = manager so not relevant \n",
    "##raw_scheduler_df = raw_scheduler_df[~((raw_scheduler_df['role'] == 2.0) | \n",
    "##                                      (raw_scheduler_df['cover_type'] == 'full'))]\n",
    "\n",
    "raw_scheduler_df['start_time_est'] = raw_scheduler_df['start_time_est'].dt.date #convert to just date \n",
    "raw_scheduler_df['cover_time_est'] = raw_scheduler_df['cover_time_est'].dt.date #convert to just date \n",
    "\n",
    "raw_scheduler_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad2223-74ad-4292-9e18-142767b96cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the coverers to a new DF\n",
    "cover_sch_df = raw_scheduler_df[raw_scheduler_df['cover_type'].isin(['before', 'after'])][['coverer_name', \n",
    "                                                                                           'coverer_email', \n",
    "                                                                                           'role', \n",
    "                                                                                           'cover_time_est']]\n",
    "cover_sch_df.rename(columns={'coverer_name': 'assignee_name', \n",
    "                             'coverer_email': 'assignee_email', \n",
    "                             'cover_time_est': 'start_time_est'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f216b-673e-4098-80e2-36766c148431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the original DF with the coverer DF to make one final shift DF\n",
    "all_sch_df = pd.concat([raw_scheduler_df, cover_sch_df], ignore_index=True)\n",
    "all_sch_df.drop(columns=['role', 'coverer_name', 'cover_type', 'cover_time_est', 'coverer_email'], inplace=True) #drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d8e1c-be5d-451f-83d2-482c5edaee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split name into first and last\n",
    "split_names = all_sch_df['assignee_name'].str.split()\n",
    "all_sch_df['First Name'] = split_names.str[0]\n",
    "all_sch_df['Last Name'] = split_names.str[1:].str.join(' ')\n",
    "\n",
    "all_sch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e7b1a-0a81-4dc6-bce9-1930be2797df",
   "metadata": {},
   "source": [
    "#### Define Timeframe Set to All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d0c91-5611-441b-9bd7-9058301a2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2023-04-01').date()\n",
    "end_date = pd.Timestamp('2023-08-01').date()\n",
    "\n",
    "sch_timeframe_df = all_sch_df[(all_sch_df['start_time_est'] >= start_date) & \n",
    "                              (all_sch_df['start_time_est'] <= end_date)]\n",
    "\n",
    "adp_timeframe_df = raw_adp_df[(raw_adp_df['Date'] >= start_date) & \n",
    "                              (raw_adp_df['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d2028-ba26-47e7-959a-205c04a5ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_timeframe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885efd1-5726-459c-b0fe-2e8899df7c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_timeframe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609bf18e-9186-4658-865a-d8868fc18ecd",
   "metadata": {},
   "source": [
    "#### Grouping - Count Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d367b2-995b-4987-a6ee-73e2b2439aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adp_grouped_df = adp_timeframe_df.groupby(['Email', 'First Name', 'Last Name']).size().reset_index(name='ADP_Count')\n",
    "adp_grouped_df = adp_grouped_df.sort_values(by='Last Name') #sort by last name\n",
    "adp_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8396d7d-70c8-4a10-a297-7f6874d949f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sch_grouped_df = sch_timeframe_df.groupby(['assignee_email', 'First Name', 'Last Name']).size().reset_index(name='Sch_Count') #find count per person\n",
    "sch_grouped_df = sch_grouped_df.sort_values(by='Last Name') #sort by last name\n",
    "sch_grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b040c7f-df8d-4293-94e9-c05a5e449363",
   "metadata": {},
   "source": [
    "#### Create Pairing Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6c400-4de0-4adb-8ae5-72e9960613be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    return name.strip().lower()\n",
    "\n",
    "def normalize_email(email):\n",
    "    return email.strip().lower()\n",
    "\n",
    "#Function to match ADP to Scheduler using a variety of techniques\n",
    "def create_merged_df8(adp_grouped_df, sch_grouped_df):\n",
    "    merged_records = []\n",
    "    unmatched_sch_records = []\n",
    "\n",
    "    for sch_index, sch_row in sch_grouped_df.iterrows():\n",
    "        normalized_assignee_email = normalize_email(sch_row['assignee_email'])\n",
    "\n",
    "        matching_email_rows = adp_grouped_df[adp_grouped_df['Email'].apply(normalize_email) == normalized_assignee_email]\n",
    "        \n",
    "        if not matching_email_rows.empty:\n",
    "            for _, matching_row in matching_email_rows.iterrows():\n",
    "                merged_records.append([\n",
    "                    matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                    sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'email'\n",
    "                ])\n",
    "        else:\n",
    "            # Check for exact first name and last name match\n",
    "            exact_name_match_rows = adp_grouped_df[\n",
    "                (adp_grouped_df['First Name'].apply(normalize_name) == normalize_name(sch_row['First Name'])) &\n",
    "                (adp_grouped_df['Last Name'].apply(normalize_name) == normalize_name(sch_row['Last Name']))\n",
    "            ]\n",
    "            \n",
    "            if not exact_name_match_rows.empty:\n",
    "                for _, matching_row in exact_name_match_rows.iterrows():\n",
    "                    merged_records.append([\n",
    "                        matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                        sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'name_exact'\n",
    "                    ])\n",
    "            else:\n",
    "                # Check for first initial and last name match or first name and last initial match\n",
    "                if sch_row['Last Name']:  # Check if 'Last Name' is not empty\n",
    "                    initial_match_rows = adp_grouped_df[\n",
    "                        ((adp_grouped_df['First Name'].str[0] == sch_row['First Name'][0]) &\n",
    "                         (adp_grouped_df['Last Name'] == sch_row['Last Name'])) |\n",
    "                        ((adp_grouped_df['First Name'] == sch_row['First Name']) &\n",
    "                         (adp_grouped_df['Last Name'].str[0] == sch_row['Last Name'][0]))\n",
    "                    ]\n",
    "                \n",
    "                    if not initial_match_rows.empty:\n",
    "                        for _, matching_row in initial_match_rows.iterrows():\n",
    "                            merged_records.append([\n",
    "                                matching_row['Email'], matching_row['First Name'], matching_row['Last Name'], matching_row['ADP_Count'],\n",
    "                                sch_row['assignee_email'], sch_row['First Name'], sch_row['Last Name'], sch_row['Sch_Count'], 'name_portion'\n",
    "                            ])\n",
    "                    else:\n",
    "                        unmatched_sch_records.append(sch_row)\n",
    "                else:\n",
    "                    unmatched_sch_records.append(sch_row)  # Handle empty 'Last Name'\n",
    "\n",
    "    # Create a DataFrame from the collected records\n",
    "    merged_df = pd.DataFrame(merged_records, columns=[\n",
    "        'Email_ADP', 'First_Name_ADP', 'Last_Name_ADP', 'ADP_Count',\n",
    "        'Email_Sch', 'First_Name_Sch', 'Last_Name_Sch', 'Sch_Count', 'Matched?'\n",
    "    ])\n",
    "    \n",
    "    # Create a DataFrame for unmatched rows from sch_grouped_df\n",
    "    unmatched_sch_df = pd.DataFrame(unmatched_sch_records, columns=sch_grouped_df.columns)\n",
    "    \n",
    "    # Append unmatched rows to the merged_df with 'none' in the 'Matched?' column\n",
    "    unmatched_rows = pd.DataFrame({\n",
    "        'Email_ADP': None,\n",
    "        'First_Name_ADP': None,\n",
    "        'Last_Name_ADP': None,\n",
    "        'ADP_Count': None,\n",
    "        'Email_Sch': unmatched_sch_df['assignee_email'],\n",
    "        'First_Name_Sch': unmatched_sch_df['First Name'],\n",
    "        'Last_Name_Sch': unmatched_sch_df['Last Name'],\n",
    "        'Sch_Count': unmatched_sch_df['Sch_Count'],\n",
    "        'Matched?': 'none'\n",
    "    })\n",
    "    merged_df = pd.concat([merged_df, unmatched_rows], ignore_index=True)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbb923-748d-412d-8b7c-40385795845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pairing table\n",
    "pairing_table = create_merged_df8(adp_grouped_df, sch_grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb55deb-20b7-48b6-9e5f-1cb5c818c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Manual changes\n",
    "email_to_change = \"dalessionicholas6@gmail.com\"\n",
    "sch_email_to_match = \"dalessnp@dukes.jmu.edu\"\n",
    "\n",
    "pairing_table.loc[pairing_table['Email_Sch'] == sch_email_to_match, 'Email_ADP'] = email_to_change\n",
    "pairing_table.loc[pairing_table['Email_Sch'] == sch_email_to_match, 'Matched?'] = 'manual'\n",
    "\n",
    "pairing_table = pairing_table[[\"Email_ADP\", \"Email_Sch\", \"Matched?\"]] #keep relevant cols\n",
    "pairing_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545e618-b1d3-4f7f-a642-39448db286e2",
   "metadata": {},
   "source": [
    "## ADOPTION CALC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659e0b9-95da-4a35-b4e1-923f2ee7745b",
   "metadata": {},
   "source": [
    "#### Read in ADP Data For Adoption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ee88c-a718-4c0a-850a-4e3d99bdc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "adp_adoption = pd.read_excel('inputs/PunchSourceEmails2023.xlsx') #read in the excel, put into DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22e4f6-eb92-44f9-b7de-b0fe8e4d25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "adp_adoption = adp_adoption[['Last Name', 'First Name', 'Personal Contact: Personal Email', \n",
    "                         'Time In', 'Pay Code [Timecard]']] #keep these cols\n",
    "adp_adoption = adp_adoption.rename(columns={'Personal Contact: Personal Email': 'Email'}) #rename col\n",
    "adp_adoption = adp_adoption[adp_adoption['Pay Code [Timecard]'] != \"REGSAL\"] #drop managers from the pay data\n",
    "adp_adoption.loc[:, 'Date'] = adp_adoption['Time In'].dt.date #make it solely a date col\n",
    "adp_adoption.drop(columns=['Time In', 'Pay Code [Timecard]'], inplace=True) #then drop orig time in col\n",
    "\n",
    "adp_adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfad57d-3b9e-421a-99d0-3c3a804b9bc2",
   "metadata": {},
   "source": [
    "#### Read in Scheduler Data For Adoption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab02fb-db33-4e21-b3d0-f53ab438eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the excel, put into DF\n",
    "scheduler_adoption = pd.read_excel('inputs/SchedulerShifts2023.xlsx') #read in the excel, put into DF\n",
    "scheduler_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177428a-5573-4dbb-9a86-14df18174fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the relevant data\n",
    "scheduler_adoption = scheduler_adoption.drop(columns=[\"epoch_start_time\", \"cover_time\"]) #drop these cols\n",
    "\n",
    "#can drop any rows that have cover_type full and role = 2 \n",
    "# b/c cover_type full either means request in future or was not taken\n",
    "# 2 = manager so not relevant \n",
    "scheduler_adoption = scheduler_adoption[~((scheduler_adoption['role'] == 2.0) | \n",
    "                                          (scheduler_adoption['cover_type'] == 'full'))]\n",
    "\n",
    "scheduler_adoption['start_time_est'] = scheduler_adoption['start_time_est'].dt.date #convert to just date \n",
    "scheduler_adoption['cover_time_est'] = scheduler_adoption['cover_time_est'].dt.date #convert to just date \n",
    "\n",
    "scheduler_adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023c8a4-f15d-45d1-b6de-ed27646ba6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the coverers to a new DF\n",
    "cover_scheduler_adoption = scheduler_adoption[scheduler_adoption['cover_type'].isin(['before', 'after'])][['coverer_name', \n",
    "                                                                                           'coverer_email', \n",
    "                                                                                           'role', \n",
    "                                                                                           'cover_time_est']]\n",
    "cover_scheduler_adoption.rename(columns={'coverer_name': 'assignee_name', \n",
    "                             'coverer_email': 'assignee_email', \n",
    "                             'cover_time_est': 'start_time_est'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a7e4cd-c3c2-4757-a069-293e6a33ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the original DF with the coverer DF to make one final shift DF\n",
    "final_scheduler_adoption = pd.concat([scheduler_adoption, cover_scheduler_adoption], ignore_index=True)\n",
    "final_scheduler_adoption.drop(columns=['role', 'coverer_name', 'cover_type', 'cover_time_est', 'coverer_email'], inplace=True) #drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b68642-d8fa-4f6d-9baf-b2c6d1dbc3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split name into first and last\n",
    "split_names = final_scheduler_adoption['assignee_name'].str.split()\n",
    "final_scheduler_adoption['First Name'] = split_names.str[0]\n",
    "final_scheduler_adoption['Last Name'] = split_names.str[1:].str.join(' ')\n",
    "\n",
    "final_scheduler_adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6e9df6-36b4-4859-b40b-6751bb4c0f6f",
   "metadata": {},
   "source": [
    "#### Define Timeframe For Adoption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3f666-bcaa-453d-9943-cbefd59dc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = pd.Timestamp('2023-06-01').date()\n",
    "end_date = pd.Timestamp('2023-06-07').date()\n",
    "\n",
    "TF_SCH = final_scheduler_adoption[(final_scheduler_adoption['start_time_est'] >= start_date) & \n",
    "                                  (final_scheduler_adoption['start_time_est'] <= end_date)]\n",
    "\n",
    "TF_ADP = adp_adoption[(adp_adoption['Date'] >= start_date) & \n",
    "                      (adp_adoption['Date'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311d169-04eb-4078-bca7-c85d7b80650b",
   "metadata": {},
   "source": [
    "#### Grouping -- Count Shifts For Adoption Calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823cb424-957c-473d-a6d2-b20bbb611fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_ADP = TF_ADP.groupby(['Email']).size().reset_index(name='ADP_Count')\n",
    "GR_ADP.rename(columns={\"Email\": \"Email_ADP\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16290f-db8d-4b08-975c-ef6aa678ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_SCH = TF_SCH.groupby(['assignee_email']).size().reset_index(name='Sch_Count') #find count per person\n",
    "GR_SCH.rename(columns={\"assignee_email\": \"Email_Sch\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3a50b-cfcb-4e2b-8058-bc6ab6cbbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = GR_SCH.merge(pairing_table, on=\"Email_Sch\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c14ad3-1802-49fd-bfd0-4170651e5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "counting = counting.merge(GR_ADP, on=\"Email_ADP\", how=\"left\")\n",
    "\n",
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe87c55-aa0b-41c5-b915-b72e329293b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Column to Measure difference in shifts\n",
    "#Sch - ADP;\n",
    "# PLUS means more SCH Shifts (relatively good)\n",
    "# MINUS means more ADP shifts (relatively bad)\n",
    "\n",
    "counting[\"Diff\"] = np.where(\n",
    "    counting[\"Sch_Count\"].notna() & counting[\"ADP_Count\"].notna(),\n",
    "    counting[\"Sch_Count\"] - counting[\"ADP_Count\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13929b3-7294-47e0-9736-26279d112360",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_within_range = counting[\"Diff\"].between(-2, 2).sum()\n",
    "count_within_range "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07487d98-ccbf-4a3b-b27f-8a965efd22b6",
   "metadata": {},
   "source": [
    "#### Calculate Adoption Rates Per Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661545c-c371-4a8b-bf77-7467c10b0114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adoption_rates_per_week(start_date, end_date, ADP_DF, SCH_DF, pairing_table):\n",
    "    \n",
    "    #Filter the main ADP and SCH dataframes to the respective timeframes\n",
    "    ADP_DF = ADP_DF[(ADP_DF['Date'] >= start_date) & \n",
    "                    (ADP_DF['Date'] <= end_date)]\n",
    "    \n",
    "    SCH_DF = SCH_DF[(SCH_DF['start_time_est'] >= start_date) & \n",
    "                    (SCH_DF['start_time_est'] <= end_date)]\n",
    "\n",
    "    #Group the ADP and SCH DFs by email\n",
    "    GR_ADP = ADP_DF.groupby(['Email']).size().reset_index(name='ADP_Count')\n",
    "    GR_ADP.rename(columns={\"Email\": \"Email_ADP\"}, inplace=True)\n",
    "    \n",
    "    GR_SCH = SCH_DF.groupby(['assignee_email']).size().reset_index(name='SCH_Count')\n",
    "    GR_SCH.rename(columns={\"assignee_email\": \"Email_Sch\"}, inplace=True)\n",
    "    \n",
    "    #Match the emails with the pairing table\n",
    "    matched_df = GR_SCH.merge(pairing_table, on=\"Email_Sch\", how=\"left\")\n",
    "    matched_df = matched_df.merge(GR_ADP, on=\"Email_ADP\", how=\"left\")\n",
    "    \n",
    "    #Create Column to Measure difference in shifts (plus means more SCH shiftS)\n",
    "    matched_df[\"Diff\"] = np.where(\n",
    "        matched_df[\"SCH_Count\"].notna() & matched_df[\"ADP_Count\"].notna(),\n",
    "        matched_df[\"SCH_Count\"] - matched_df[\"ADP_Count\"],\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    #Calculate the yes/partial/no counts\n",
    "    adoption_yes_count = matched_df[\"Diff\"].between(-2, 2).sum()\n",
    "    adoption_partial_count = matched_df.shape[0] - adoption_yes_count\n",
    "    adoption_no_count = GR_ADP.shape[0] - adoption_yes_count - adoption_partial_count\n",
    "    sum_count = adoption_yes_count + adoption_partial_count + adoption_no_count\n",
    "    \n",
    "    #Calculate the yes/partial/no percents\n",
    "    yes_perc = round(adoption_yes_count / sum_count, 4)\n",
    "    partial_perc = round(adoption_partial_count / sum_count, 4)\n",
    "    no_perc = round(adoption_no_count / sum_count, 4)\n",
    "    \n",
    "    #Format start/end date\n",
    "    formatted_start_date = start_date.strftime(\"%Y-%m-%d\")\n",
    "    formatted_end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    return [formatted_start_date, formatted_end_date, yes_perc, partial_perc, no_perc, sum_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5647d1-e6a2-4ecc-a73c-33fec9ce5e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "date_ranges = [\n",
    "    (pd.Timestamp('2023-06-01').date(), pd.Timestamp('2023-06-07').date()),\n",
    "    (pd.Timestamp('2023-06-08').date(), pd.Timestamp('2023-06-14').date()),\n",
    "    (pd.Timestamp('2023-06-15').date(), pd.Timestamp('2023-06-21').date()),\n",
    "    (pd.Timestamp('2023-06-22').date(), pd.Timestamp('2023-06-28').date()),\n",
    "    (pd.Timestamp('2023-06-29').date(), pd.Timestamp('2023-07-05').date()),\n",
    "    (pd.Timestamp('2023-07-06').date(), pd.Timestamp('2023-07-12').date()),\n",
    "    (pd.Timestamp('2023-07-13').date(), pd.Timestamp('2023-07-19').date()),\n",
    "    (pd.Timestamp('2023-07-20').date(), pd.Timestamp('2023-07-26').date()),\n",
    "    (pd.Timestamp('2023-07-27').date(), pd.Timestamp('2023-08-02').date()),\n",
    "]\n",
    "\n",
    "\n",
    "for start_date, end_date in date_ranges:\n",
    "    week_result = adoption_rates_per_week(start_date, \n",
    "                                          end_date, \n",
    "                                          adp_adoption, \n",
    "                                          final_scheduler_adoption, \n",
    "                                          pairing_table)\n",
    "    results.append(week_result)\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=[\"Start Date\", \"End Date\", \"Yes %\", \"Partial %\", \"No %\", \"Sum Users\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f88fdc-7d7d-4310-8d50-337a54d1dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f841b3-7cf7-404e-bdce-a4135fa7d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Melt the DataFrame to make it suitable for plotting\n",
    "melted_df = result_df.melt(id_vars=['Start Date', 'End Date'], value_vars=['Yes %', 'Partial %', 'No %'],\n",
    "                    var_name='Percentage', value_name='Value')\n",
    "\n",
    "# Define custom hex colors for each percentage category\n",
    "color_map = {\n",
    "    'Yes %': '#44B699',\n",
    "    'Partial %': '#FFE79A',\n",
    "    'No %': '#EDEDED',\n",
    "}\n",
    "\n",
    "# Create the stacked bar chart using Plotly Express with custom colors\n",
    "fig = px.bar(melted_df, x='Start Date', y='Value', color='Percentage',\n",
    "             title=\"Stacked Bar Chart of Yes%, Partial%, and No%\",\n",
    "             labels={'Value': 'Percentage'},\n",
    "             color_discrete_map=color_map,  # Use custom color map\n",
    "             height=500)\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis_tickformat='.0%',\n",
    "    hovermode='x'\n",
    ")\n",
    "\n",
    "# Customize hover behavior\n",
    "hover_template = '<b>%{x}</b><br>%{y:.0%}<extra></extra>'\n",
    "fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "\n",
    "# Lighten the colors of other bars when hovering over yellow section\n",
    "for color_key in color_map:\n",
    "    color_value = color_map[color_key]\n",
    "    fig.update_traces(selector=dict(marker_color=[color_value]),\n",
    "                      unselected=dict(marker_opacity=0.3))\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2452069-9257-47c7-87fd-d0eaa8038787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Define custom hex colors for each percentage category\n",
    "color_map = {\n",
    "    'Yes %': '#44B699',\n",
    "    'Partial %': '#FFE79A',\n",
    "    'No %': '#EDEDED',\n",
    "}\n",
    "\n",
    "# Create the stacked bar chart using Plotly Graph Objects\n",
    "fig = go.Figure()\n",
    "\n",
    "for percentage in ['Yes %', 'Partial %', 'No %']:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=result_df['Start Date'],\n",
    "        y=result_df[percentage],\n",
    "        name=percentage,\n",
    "        hoverinfo='y+name',\n",
    "        marker_color=color_map[percentage]\n",
    "    ))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    plot_bgcolor='white',\n",
    "    yaxis_tickformat='.0%',\n",
    "    hovermode='x',\n",
    "    height = 700\n",
    ")\n",
    "\n",
    "# Customize hover behavior\n",
    "hover_template = '%{y:.1%}<extra></extra>'  # Display y-axis value with one decimal place\n",
    "fig.update_traces(hovertemplate=hover_template)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec439861-0eb1-423b-a455-1588cc371ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the interactive HTML representation of the plot\n",
    "interactive_html = fig.to_html(full_html=False)\n",
    "\n",
    "# Display the interactive HTML (optional, for testing)\n",
    " display(HTML(interactive_html))\n",
    "\n",
    "# Save the interactive HTML to a file\n",
    "#with open(\"stacked_bar_chart.html\", \"w\") as f:\n",
    "#    f.write(interactive_html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
